{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c846469-154d-46d7-bd13-a98cd94e20fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('PCmaster_anno_0_23_12_24.py','r',encoding='utf-8') as f:\n",
    "    exec(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c4b0c-7a31-4267-a682-2cbdf0307c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset2(Dataset):  \n",
    "    def __init__(self, root_dir, transform=None):  \n",
    "        self.root_dir = root_dir  \n",
    "        self.classes = sorted(os.listdir(root_dir))  \n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}  \n",
    "        self.samples = self._make_dataset()  \n",
    "        self.transform = transforms.Compose([  \n",
    "            transforms.Resize((224, 224)),  # 224x224  \n",
    "            transforms.ToTensor()  # to tensor\n",
    "        ])   \n",
    "  \n",
    "    def _make_dataset(self):  \n",
    "        samples = []  \n",
    "        for class_name in self.classes:  \n",
    "            class_dir = os.path.join(self.root_dir, class_name)  \n",
    "            if not os.path.isdir(class_dir):  \n",
    "                continue  \n",
    "            for file_name in os.listdir(class_dir):  \n",
    "                file_path = os.path.join(class_dir, file_name)  \n",
    "                samples.append((file_path, self.class_to_idx[class_name]))  \n",
    "        return samples  \n",
    "  \n",
    "    def __len__(self):  \n",
    "        return len(self.samples)  \n",
    "  \n",
    "    def __getitem__(self, idx):  \n",
    "        file_path, label = self.samples[idx]  \n",
    "        image = torch.load(file_path)  \n",
    "        return image, label  \n",
    "\n",
    "class Dataset_ResNet_2(Dataset):  \n",
    "    def __init__(self,data,label):\n",
    "        self.Data = torch.from_numpy(np.float32(np.array(data)))\n",
    "        new_tensor = torch.zeros(self.Data.size(0), 3*224*224)\n",
    "        new_tensor[:, :self.Data.size(1)] = self.Data\n",
    "        self.Data = new_tensor\n",
    "        self.Data = self.Data.view(self.Data.size(0), 3, 224, 224)\n",
    "        self.Label = torch.from_numpy(np.int64(np.array(label)).reshape(1,-1)[0])\n",
    "    def __getitem__(self, index):\n",
    "        x = self.Data[index]\n",
    "        y = self.Label[index]\n",
    "        return x, y  \n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "        \n",
    "class PscResNet2(nn.Module):\n",
    "    def __init__(self, dropout_ratio1=0.1, dropout_ratio2=0.1, dropout_ratio3=0.1, num_types=None):\n",
    "        super(PscResNet2, self).__init__()\n",
    "        self.dropout_layer1 = nn.Dropout(dropout_ratio1)\n",
    "        self.resnet18 = torchvision.models.resnet18(weights=None)   \n",
    "        self.pscMLP = nn.Sequential(\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dropout_ratio2),\n",
    "                        nn.Linear(1000, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dropout_ratio3),\n",
    "                        nn.Linear(512, num_types),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout_layer1(x)\n",
    "        x = self.resnet18(x)\n",
    "        x = self.pscMLP(x)\n",
    "        return x\n",
    "\n",
    "class Dataset_ResNet(Dataset):  \n",
    "    def __init__(self,data,label):\n",
    "        self.Data = torch.from_numpy(np.float32(np.array(data)))\n",
    "        self.Data = torch.reshape(self.Data, (data.shape[0], 224, 224))\n",
    "        self.Data = torch.unsqueeze(self.Data, 1)\n",
    "        self.Data = torch.cat((self.Data, self.Data, self.Data), dim=1)\n",
    "        self.Label = torch.from_numpy(np.int64(np.array(label)).reshape(1,-1)[0])\n",
    "    def __getitem__(self, index):\n",
    "        x = self.Data[index]\n",
    "        y = self.Label[index]\n",
    "        return x, y  \n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "\n",
    "class PscResNet(nn.Module):\n",
    "    def __init__(self, dropout_ratio1=0.3, dropout_ratio2=0.1, dropout_ratio3=0.1, num_types=None):\n",
    "        super(PscResNet, self).__init__()\n",
    "        self.resnet18 = torchvision.models.resnet18(pretrained=False)   \n",
    "        self.pscMLP = nn.Sequential(\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dropout_ratio1),\n",
    "                        nn.Linear(1000, num_types))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet18(x)\n",
    "        x = self.pscMLP(x)\n",
    "        return x\n",
    "\n",
    "class Dataset_MLP(Dataset):  \n",
    "    def __init__(self,data,label):\n",
    "        self.Data = torch.from_numpy(np.float32(np.array(data)))\n",
    "        self.Label = torch.from_numpy(np.int64(np.array(label)).reshape(1,-1)[0])\n",
    "    def __getitem__(self, index):\n",
    "        x = self.Data[index]\n",
    "        y = self.Label[index]\n",
    "        return x, y  \n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "\n",
    "class PscMLP(nn.Module):\n",
    "    def __init__(self, feature_len=None, dropout_ratio1=0.1, dropout_ratio2=0.1, dropout_ratio3=0.1, num_classes=None):\n",
    "        super().__init__() \n",
    "        self.pscMLP = nn.Sequential(nn.Flatten(),\n",
    "                               nn.Dropout(dropout_ratio1),\n",
    "                               nn.Linear( feature_len, feature_len//4 ),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Dropout(dropout_ratio2),\n",
    "                               nn.Linear( feature_len//4, feature_len//16 ),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Dropout(dropout_ratio3),\n",
    "                               nn.Linear(feature_len//16, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pscMLP(x)\n",
    "        return x\n",
    "\n",
    "class Dataset_transformer(Dataset):  \n",
    "    def __init__(self,data,label):\n",
    "        self.Data = torch.from_numpy(np.float32(np.array(data)))\n",
    "        self.Data = torch.reshape(self.Data, (data.shape[0], 2, 1024))\n",
    "        self.Label = torch.from_numpy(np.int64(np.array(label)).reshape(1,-1)[0])\n",
    "    def __getitem__(self, index):\n",
    "        x = self.Data[index]\n",
    "        y = self.Label[index]\n",
    "        return x, y  \n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CIForm(nn.Module):\n",
    "    def __init__(self, input_dim, nhead=2, d_model=80, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, dim_feedforward=1024, nhead=nhead, dropout=dropout\n",
    "        )\n",
    "        self.positionalEncoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.permute(1, 0, 2)\n",
    "        out = self.positionalEncoding(out)\n",
    "        out = self.encoder_layer(out)\n",
    "        out = out.transpose(0, 1)\n",
    "        out = out.mean(dim=1)\n",
    "        out = self.pred_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e4c71-a03c-4167-a1d9-5755658519df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed=None):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def print_memory_used():\n",
    "    process = psutil.Process()  \n",
    "    memory_usage = process.memory_info().rss / 1024 / 1024 /1024 \n",
    "    print(f\"memory used: {memory_usage} GB\")  \n",
    "\n",
    "def print_anndata(input_adata=None):\n",
    "    new_anndata = input_adata\n",
    "    print(new_anndata)\n",
    "    print(new_anndata.obs_names)\n",
    "    print(new_anndata.var_names)\n",
    "    print(new_anndata.X.max())\n",
    "    print(new_anndata.obs)\n",
    "    print(new_anndata.var)\n",
    "\n",
    "def preprocessing(input_adata=None,if_filter_genes=True,if_norm=True,if_hvg=False,n_top_hvgs=None):\n",
    "    new_anndata = input_adata\n",
    "    if if_filter_genes==True:\n",
    "        sc.pp.filter_genes(new_anndata, min_cells=1)\n",
    "        print('Genes filtered.')\n",
    "    if if_norm==True:\n",
    "        if new_anndata.X.max() > 50:\n",
    "            sc.pp.normalize_total(new_anndata, target_sum=1e4)\n",
    "            sc.pp.log1p(new_anndata)\n",
    "            print('Data normalized.')\n",
    "    new_anndata.var_names_make_unique()\n",
    "    new_anndata.obs_names_make_unique()\n",
    "    print('Names are changed to be unique.')\n",
    "    if if_hvg==True:\n",
    "        if n_top_hvgs is not None:\n",
    "            sc.pp.highly_variable_genes(new_anndata,n_top_genes=2048)\n",
    "        else:\n",
    "            sc.pp.highly_variable_genes(new_anndata)\n",
    "        sc.pl.highly_variable_genes(new_anndata)\n",
    "        new_anndata = new_anndata[:, new_anndata.var.highly_variable]\n",
    "        print('Hvgs selected.')\n",
    "    return new_anndata\n",
    "\n",
    "def split_anndata_by_celltype_to_train_valid_test(new_anndata=None, celltype_label='celltype', split_ratio=[0.8,0.1,0.1], random_seed=42):  \n",
    "    # split adata to three part by given ratio\n",
    "    np.random.seed(random_seed)\n",
    "    train_anndata = None\n",
    "    valid_anndata = None\n",
    "    test_anndata = None\n",
    "    \n",
    "    for celltype in new_anndata.obs[celltype_label].unique(): \n",
    "        celltype_indices = new_anndata.obs[celltype_label] == celltype  \n",
    "        celltype_data = new_anndata[celltype_indices]  \n",
    "        \n",
    "        total_cells = len(celltype_data)  \n",
    "        x, y, z = split_ratio  \n",
    "        x_count = int(total_cells * x / (x + y + z))  \n",
    "        y_count = int(total_cells * y / (x + y + z))  \n",
    "        z_count = total_cells - x_count - y_count  \n",
    "        \n",
    "        indices = np.random.permutation(total_cells)  \n",
    "        train_indices = indices[:x_count]  \n",
    "        valid_indices = indices[x_count:x_count + y_count]  \n",
    "        test_indices = indices[x_count + y_count:]  \n",
    "        \n",
    "        train_data = celltype_data[train_indices]  \n",
    "        valid_data = celltype_data[valid_indices]  \n",
    "        test_data = celltype_data[test_indices]  \n",
    "        \n",
    "        if train_anndata is None:\n",
    "            train_anndata = train_data\n",
    "        else:\n",
    "            train_anndata = train_anndata.concatenate(train_data, index_unique=None)  \n",
    "        if valid_anndata is None:\n",
    "            valid_anndata = valid_data\n",
    "        else:\n",
    "            valid_anndata = valid_anndata.concatenate(valid_data, index_unique=None)   \n",
    "        if test_anndata is None:\n",
    "            test_anndata = test_data\n",
    "        else:\n",
    "            test_anndata = test_anndata.concatenate(test_data, index_unique=None)   \n",
    "        \n",
    "    return train_anndata, valid_anndata, test_anndata\n",
    "\n",
    "def generate_datas_and_labels(input_adatas=None, the_length=None, if_balance_train_data=None, random_sample_num=None):\n",
    "    new_anndata_train = input_adatas[0]\n",
    "    new_anndata_valid = input_adatas[1]\n",
    "    new_anndata_test = input_adatas[2]\n",
    "    pcma = PCmaster_anno_0()\n",
    "    train_data,mapping_1,mapping_2 = pcma.auto_annotation_with_deep_learning_transfer_adata_to_df_for_all_0(\n",
    "        adata_input = new_anndata_train, the_length=the_length)\n",
    "    valid_data,_,_ = pcma.auto_annotation_with_deep_learning_transfer_adata_to_df_for_all_0(\n",
    "        adata_input = new_anndata_valid, the_length=the_length)\n",
    "    test_data,_,_ = pcma.auto_annotation_with_deep_learning_transfer_adata_to_df_for_all_0(\n",
    "        adata_input = new_anndata_test, the_length=the_length)\n",
    "    \n",
    "    # displays the number of categories and the number of each category.\n",
    "    category_counts = train_data['celltype'].value_counts()\n",
    "    print(\"the number of categories: \", len(category_counts))\n",
    "    print(\"the number of each category: \")\n",
    "    print(category_counts)\n",
    "    print(mapping_1)\n",
    "    print(mapping_2)\n",
    "    \n",
    "    if if_balance_train_data is True:\n",
    "        train_data_balanced = pd.DataFrame()\n",
    "        for category in category_counts.index:\n",
    "            category_data = train_data[train_data['celltype'] == category]\n",
    "            print(category)\n",
    "            print(category_data.shape[0])\n",
    "            # print(type(category_data.shape[0]))\n",
    "            category_data = category_data.sample(n=random_sample_num, random_state=1, replace=True)\n",
    "            train_data_balanced = pd.concat([train_data_balanced, category_data])\n",
    "        train_data = train_data_balanced\n",
    "    \n",
    "    print_memory_used() \n",
    "    \n",
    "    train_label = train_data.iloc[:, -1:]\n",
    "    train_data = train_data.iloc[:, :-2]\n",
    "    valid_label = valid_data.iloc[:, -1:]\n",
    "    valid_data = valid_data.iloc[:, :-2]\n",
    "    test_label = test_data.iloc[:, -1:]\n",
    "    test_data = test_data.iloc[:, :-2]\n",
    "    \n",
    "    return train_data,train_label,valid_data,valid_label,test_data,test_label,mapping_1,mapping_2,category_counts\n",
    "\n",
    "# GPU\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None): \n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "# GPU\n",
    "def train_with_GPU(net=None, train_iter=None, valid_iter=None, num_epochs=None, lr=None, device=None, optimizer='Adam', momentum=None):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    if optimizer == 'SGD':\n",
    "        if momentum is not None:\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150, eta_min=0.00001)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],ylim=[0,1],\n",
    "                            legend=['train loss', 'train acc', 'valid acc'])\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    best_model_weights = None\n",
    "    best_valid_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        valid_acc = evaluate_accuracy_gpu(net, valid_iter)\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            best_epoch = epoch\n",
    "            best_model_weights = net.state_dict()\n",
    "        animator.add(epoch + 1, (None, None, valid_acc))\n",
    "    net.load_state_dict(best_model_weights)\n",
    "    print(f'At the epoch {best_epoch}, the model got the best valid accuracy {best_valid_acc}.')\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'valid acc {valid_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28eaa68-b70b-4d6c-98f9-5c9f2930522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_precision_recall_f1_score(test_label=None,predicted=None):\n",
    "    accuracy = accuracy_score(test_label, predicted)  \n",
    "    # macro precision \n",
    "    macro_precision = precision_score(test_label, predicted, average='macro', zero_division=1)  \n",
    "    \n",
    "    # macro recall  \n",
    "    macro_recall = recall_score(test_label, predicted, average='macro', zero_division=1)  \n",
    "    \n",
    "    # macro F1 score\n",
    "    macro_f1_score = f1_score(test_label, predicted, average='macro', zero_division=1)  \n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)  \n",
    "    print(\"Macro Precision:\", macro_precision)  \n",
    "    print(\"Macro Recall:\", macro_recall)  \n",
    "    print(\"Macro F1 Score:\", macro_f1_score)\n",
    "    \n",
    "    return accuracy, macro_precision, macro_recall, macro_f1_score\n",
    "\n",
    "def train_ResNet2(\n",
    "    train_data=None,train_label=None,valid_data=None,valid_label=None,test_data=None,test_label=None,\n",
    "    the_batch_size=None,num_classes=None,lr=None,dropout_ratio1=None,dropout_ratio2=None,dropout_ratio3=None,\n",
    "    epochs=None,gpu_code=None,the_num_workers=None,optimizer=None, momentum=None,\n",
    "):\n",
    "    \n",
    "    traindata = Dataset_ResNet_2(train_data,train_label) \n",
    "    validdata = Dataset_ResNet_2(valid_data,valid_label)\n",
    "    \n",
    "    train_sampler = RandomSampler(traindata)\n",
    "    train_iter = DataLoader(traindata,batch_size=the_batch_size,shuffle=False,sampler=train_sampler,\n",
    "                            num_workers=the_num_workers)\n",
    "    valid_sampler = RandomSampler(validdata)\n",
    "    valid_iter = DataLoader(validdata,batch_size=the_batch_size,shuffle=False,sampler=valid_sampler,\n",
    "                            num_workers=the_num_workers)\n",
    "    \n",
    "    del train_data,train_label,valid_data,valid_label,traindata,validdata\n",
    "    gc.collect()\n",
    "    print_memory_used()\n",
    "    print(f'num_classes = {num_classes}')\n",
    "    \n",
    "    pscresnet = PscResNet2(\n",
    "        dropout_ratio1=dropout_ratio1, dropout_ratio2=dropout_ratio2, dropout_ratio3=dropout_ratio3, num_types=num_classes,\n",
    "    )\n",
    "    \n",
    "    torch.cuda.init()\n",
    "    # net=None, train_iter=None, valid_iter=None, num_epochs=None, lr=None, device=None, optimizer='Adam', momentum=None\n",
    "    train_with_GPU(net=pscresnet, train_iter=train_iter, valid_iter=valid_iter, \n",
    "                   num_epochs=epochs, lr=lr, device=d2l.try_all_gpus()[gpu_code], optimizer=optimizer, momentum=momentum)\n",
    "    model = pscresnet\n",
    "    del train_sampler,train_iter,valid_sampler,valid_iter\n",
    "    gc.collect()\n",
    "    print_memory_used()\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    batch_size = 32 \n",
    "    total_size = test_data.shape[0]\n",
    "    total_batches = math.ceil(total_size / batch_size)\n",
    "    \n",
    "    net_try = model\n",
    "    net_try = net_try.to('cpu')\n",
    "    net_try.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_results = []\n",
    "        for i in range(total_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, total_size)\n",
    "            batch_data = test_data[start:end]\n",
    "            batch_label = test_label[start:end]\n",
    "            batch_data = torch.from_numpy(np.float32(np.array(batch_data)))\n",
    "            new_tensor = torch.zeros(batch_data.size(0), 3*224*224)\n",
    "            new_tensor[:, :batch_data.size(1)] = batch_data\n",
    "            batch_data = new_tensor\n",
    "            batch_data = batch_data.view(batch_data.size(0), 3, 224, 224)\n",
    "            batch_label = torch.from_numpy(np.int64(np.array(batch_label)).reshape(1,-1)[0])\n",
    "            outputs = net_try(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_results.append(predicted.numpy())\n",
    "            \n",
    "    predicted = np.concatenate(predicted_results)\n",
    "    test_label = torch.from_numpy(np.int64(np.array(test_label)).reshape(1,-1)[0])\n",
    "    test_label = test_label.numpy()\n",
    "    \n",
    "    accuracy,macro_precision,macro_recall,macro_f1_score = get_accuracy_precision_recall_f1_score(\n",
    "        test_label=test_label,predicted=predicted)\n",
    "\n",
    "    return model, accuracy, macro_precision, macro_recall, macro_f1_score\n",
    "\n",
    "def train_ResNet(\n",
    "    train_data=None,train_label=None,valid_data=None,valid_label=None,test_data=None,test_label=None,\n",
    "    the_batch_size=None,num_classes=None,lr=None,dropout_ratio1=None,dropout_ratio2=None,dropout_ratio3=None,\n",
    "    epochs=None,gpu_code=None,the_num_workers=None,optimizer=None, momentum=None,\n",
    "):\n",
    "    \n",
    "    traindata = Dataset_ResNet(train_data,train_label) \n",
    "    validdata = Dataset_ResNet(valid_data,valid_label)\n",
    "    \n",
    "    train_sampler = RandomSampler(traindata)\n",
    "    train_iter = DataLoader(traindata,batch_size=the_batch_size,shuffle=False,sampler=train_sampler,\n",
    "                            num_workers=the_num_workers)\n",
    "    valid_sampler = RandomSampler(validdata)\n",
    "    valid_iter = DataLoader(validdata,batch_size=the_batch_size,shuffle=False,sampler=valid_sampler,\n",
    "                            num_workers=the_num_workers)\n",
    "    \n",
    "    del train_data,train_label,valid_data,valid_label,traindata,validdata\n",
    "    gc.collect()\n",
    "    print_memory_used()\n",
    "    print(f'num_classes = {num_classes}')\n",
    "    \n",
    "    pscresnet = PscResNet(\n",
    "        dropout_ratio1=dropout_ratio1, dropout_ratio2=dropout_ratio2, dropout_ratio3=dropout_ratio3, num_types=num_classes,\n",
    "    )\n",
    "    pretrained_dict = torch.load('resnet18.pth')\n",
    "    model_dict = pscresnet.resnet18.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    pscresnet.resnet18.load_state_dict(model_dict)\n",
    "    torch.cuda.init()\n",
    "    \n",
    "    # net=None, train_iter=None, valid_iter=None, num_epochs=None, lr=None, device=None, optimizer='Adam', momentum=None\n",
    "    train_with_GPU(net=pscresnet, train_iter=train_iter, valid_iter=valid_iter, \n",
    "                   num_epochs=epochs, lr=lr, device=d2l.try_all_gpus()[gpu_code], optimizer=optimizer, momentum=momentum)\n",
    "    model = pscresnet\n",
    "    del train_sampler,train_iter,valid_sampler,valid_iter\n",
    "    gc.collect()\n",
    "    print_memory_used()\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    batch_size = 32 \n",
    "    total_size = test_data.shape[0]\n",
    "    total_batches = math.ceil(total_size / batch_size)\n",
    "    \n",
    "    net_try = model\n",
    "    net_try = net_try.to('cpu')\n",
    "    net_try.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_results = []\n",
    "        for i in range(total_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, total_size)\n",
    "            batch_data = test_data[start:end]\n",
    "            batch_label = test_label[start:end]\n",
    "            batch_data = torch.from_numpy(np.float32(np.array(batch_data)))\n",
    "            batch_data = torch.reshape(batch_data, (batch_data.shape[0], 224, 224))\n",
    "            batch_data = torch.unsqueeze(batch_data, 1)\n",
    "            batch_data = torch.cat((batch_data, batch_data, batch_data), dim=1)\n",
    "            batch_label = torch.from_numpy(np.int64(np.array(batch_label)).reshape(1,-1)[0])\n",
    "            outputs = net_try(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_results.append(predicted.numpy())\n",
    "            \n",
    "    predicted = np.concatenate(predicted_results)\n",
    "    test_label = torch.from_numpy(np.int64(np.array(test_label)).reshape(1,-1)[0])\n",
    "    test_label = test_label.numpy()\n",
    "    \n",
    "    accuracy,macro_precision,macro_recall,macro_f1_score = get_accuracy_precision_recall_f1_score(\n",
    "        test_label=test_label,predicted=predicted)\n",
    "\n",
    "    return model, accuracy, macro_precision, macro_recall, macro_f1_score\n",
    "\n",
    "def train_SVM(\n",
    "    train_data=None,train_label=None,valid_data=None,valid_label=None,test_data=None,test_label=None,\n",
    "):\n",
    "    Classifier = LinearSVC()\n",
    "    clf = CalibratedClassifierCV(Classifier)\n",
    "    start_time = time.time()\n",
    "    clf.fit(train_data, train_label)\n",
    "    model = clf\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"time cost：{execution_time} s\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predicted = clf.predict(test_data)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"time cost：{execution_time} s\")\n",
    "    \n",
    "    accuracy,macro_precision,macro_recall,macro_f1_score = get_accuracy_precision_recall_f1_score(\n",
    "        test_label=test_label,predicted=predicted)\n",
    "    \n",
    "    return model, accuracy, macro_precision, macro_recall, macro_f1_score\n",
    "\n",
    "def train_MLP(\n",
    "    train_data=None,train_label=None,valid_data=None,valid_label=None,test_data=None,test_label=None,\n",
    "    the_batch_size=None,num_classes=None,lr=None,dropout_ratio1=None,dropout_ratio2=None,dropout_ratio3=None,feature_len=None,\n",
    "    epochs=None,gpu_code=None,the_num_workers=None,optimizer=None, momentum=None,\n",
    "):\n",
    "    \n",
    "    traindata = Dataset_MLP(train_data,train_label) \n",
    "    validdata = Dataset_MLP(valid_data,valid_label)\n",
    "    \n",
    "    train_sampler = RandomSampler(traindata)\n",
    "    train_iter = DataLoader(traindata,batch_size=the_batch_size,shuffle=False,sampler=train_sampler,\n",
    "                            num_workers=the_num_workers)\n",
    "    valid_sampler = RandomSampler(validdata)\n",
    "    valid_iter = DataLoader(validdata,batch_size=the_batch_size,shuffle=False,sampler=valid_sampler,\n",
    "                            num_workers=the_num_workers)\n",
    "    \n",
    "    del train_data,train_label,valid_data,valid_label,traindata,validdata\n",
    "    gc.collect()\n",
    "    print_memory_used()\n",
    "    print(f'num_classes = {num_classes}')\n",
    "    \n",
    "    pscmlp = PscMLP(\n",
    "        feature_len=feature_len,\n",
    "        dropout_ratio1=dropout_ratio1, dropout_ratio2=dropout_ratio2, dropout_ratio3=dropout_ratio3, \n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    \n",
    "    torch.cuda.init()\n",
    "    # net=None, train_iter=None, valid_iter=None, num_epochs=None, lr=None, device=None, optimizer='Adam', momentum=None\n",
    "    train_with_GPU(net=pscmlp, train_iter=train_iter, valid_iter=valid_iter, \n",
    "                   num_epochs=epochs, lr=lr, device=d2l.try_all_gpus()[gpu_code], optimizer=optimizer, momentum=momentum)\n",
    "    model = pscmlp\n",
    "    del train_sampler,train_iter,valid_sampler,valid_iter\n",
    "    gc.collect()\n",
    "    print_memory_used()\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    batch_size = 32 \n",
    "    total_size = test_data.shape[0]\n",
    "    total_batches = math.ceil(total_size / batch_size)\n",
    "    \n",
    "    net_try = model\n",
    "    net_try = net_try.to('cpu')\n",
    "    net_try.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_results = []\n",
    "        for i in range(total_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, total_size)\n",
    "            batch_data = test_data[start:end]\n",
    "            batch_label = test_label[start:end]\n",
    "            batch_data = torch.from_numpy(np.float32(np.array(batch_data)))\n",
    "            batch_label = torch.from_numpy(np.int64(np.array(batch_label)).reshape(1,-1)[0])\n",
    "            outputs = net_try(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_results.append(predicted.numpy())\n",
    "            \n",
    "    predicted = np.concatenate(predicted_results)\n",
    "    test_label = torch.from_numpy(np.int64(np.array(test_label)).reshape(1,-1)[0])\n",
    "    test_label = test_label.numpy()\n",
    "    \n",
    "    accuracy,macro_precision,macro_recall,macro_f1_score = get_accuracy_precision_recall_f1_score(\n",
    "        test_label=test_label,predicted=predicted)\n",
    "\n",
    "    return model, accuracy, macro_precision, macro_recall, macro_f1_score\n",
    "\n",
    "def train_CIForm(\n",
    "    train_data=None,train_label=None,valid_data=None,valid_label=None,test_data=None,test_label=None,\n",
    "    the_batch_size=None,num_classes=None,lr=None,dropout_ratio1=None,dropout_ratio2=None,dropout_ratio3=None,feature_len=None,\n",
    "    epochs=None,gpu_code=None,the_num_workers=None,the_num_heads=None,optimizer=None, momentum=None,\n",
    "):\n",
    "    traindata = Dataset_transformer(train_data,train_label) \n",
    "    validdata = Dataset_transformer(valid_data,valid_label)\n",
    "    \n",
    "    train_sampler = RandomSampler(traindata)\n",
    "    train_iter = DataLoader(traindata,batch_size=the_batch_size,shuffle=False,sampler=train_sampler,\n",
    "                            num_workers=the_num_workers)\n",
    "    valid_sampler = RandomSampler(validdata)\n",
    "    valid_iter = DataLoader(validdata,batch_size=the_batch_size,shuffle=False,sampler=valid_sampler,\n",
    "                            num_workers=the_num_workers)\n",
    "    del traindata,train_data,train_label,validdata,valid_data,valid_label\n",
    "    gc.collect() \n",
    "    print_memory_used()\n",
    "    \n",
    "    s = feature_len//2\n",
    "    heads = the_num_heads\n",
    "    dp = dropout_ratio1\n",
    "    batch_sizes = the_batch_size\n",
    "    num_classes = num_classes\n",
    "    print(f'num_classes = {num_classes}')\n",
    "    \n",
    "    ciform = CIForm(input_dim=s, nhead=heads, d_model=s,\n",
    "                       num_classes=num_classes,dropout=dp)\n",
    "    \n",
    "    torch.cuda.init()\n",
    "    # net=None, train_iter=None, valid_iter=None, num_epochs=None, lr=None, device=None, optimizer='Adam', momentum=None\n",
    "    train_with_GPU(net=ciform, train_iter=train_iter, valid_iter=valid_iter, \n",
    "                   num_epochs=epochs, lr=lr, device=d2l.try_all_gpus()[gpu_code], optimizer=optimizer, momentum=momentum)\n",
    "    model = ciform\n",
    "    del train_sampler,train_iter,valid_sampler,valid_iter\n",
    "    gc.collect()\n",
    "    print_memory_used()\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    batch_size = 32 \n",
    "    total_size = test_data.shape[0]\n",
    "    total_batches = math.ceil(total_size / batch_size)\n",
    "    \n",
    "    net_try = model\n",
    "    net_try = net_try.to('cpu')\n",
    "    net_try.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_results = []\n",
    "        for i in range(total_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, total_size)\n",
    "            batch_data = test_data[start:end]\n",
    "            batch_label = test_label[start:end]\n",
    "            batch_data = torch.from_numpy(np.float32(np.array(batch_data)))\n",
    "            batch_data = torch.reshape(batch_data, (batch_data.shape[0], 2, 1024))\n",
    "            batch_label = torch.from_numpy(np.int64(np.array(batch_label)).reshape(1,-1)[0])\n",
    "            outputs = net_try(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_results.append(predicted.numpy())\n",
    "    predicted = np.concatenate(predicted_results)\n",
    "    test_label = torch.from_numpy(np.int64(np.array(test_label)).reshape(1,-1)[0])\n",
    "    test_label = test_label.numpy()\n",
    "    testlabel = test_label\n",
    "    \n",
    "    accuracy,macro_precision,macro_recall,macro_f1_score = get_accuracy_precision_recall_f1_score(\n",
    "        test_label=test_label,predicted=predicted)\n",
    "    \n",
    "    return model, accuracy, macro_precision, macro_recall, macro_f1_score\n",
    "\n",
    "def train_union():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac9445-dfdc-43c2-ab14-b571c335c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet2\n",
    "\n",
    "with open('PCmaster_anno_0_23_12_24.py','r',encoding='utf-8') as f:\n",
    "    exec(f.read())\n",
    "    \n",
    "renames = []\n",
    "reorgans = []\n",
    "renumofcells = []\n",
    "renumofcelltypes = []\n",
    "rerandomseeds = []\n",
    "reaccuracys = []\n",
    "remacrof1scores = []\n",
    "root_dir = '../scplantdb-max-1500-per-cell-type/'\n",
    "files = os.listdir(root_dir)\n",
    "filepath_list = None\n",
    "random_seeds = range(40,56)\n",
    "if_hvg=False\n",
    "n_top_hvgs=None\n",
    "celltype_label = 'Celltype'\n",
    "organ_label = 'Organ'\n",
    "split_ratio=[0.8,0.1,0.1]\n",
    "if_df = True\n",
    "if_feature_len = False # max feature_len is 3*224*224\n",
    "if_balance_train_data = True\n",
    "random_sample_num = 1500\n",
    "the_batch_size=256\n",
    "lr=0.0004\n",
    "dropout_ratio1=0.1\n",
    "dropout_ratio2=0.1\n",
    "dropout_ratio3=0.1\n",
    "epochs=20\n",
    "gpu_code=7\n",
    "the_num_workers=0\n",
    "the_optimizer='Adam'\n",
    "the_momentum=None\n",
    "\n",
    "for random_seed in random_seeds:\n",
    "    print(f'random_seed = {random_seed}')\n",
    "    same_seeds(seed = random_seed)\n",
    "    for file in files:\n",
    "        try:\n",
    "            if not file.endswith('.h5ad'):\n",
    "                continue\n",
    "            file_path = root_dir+file\n",
    "            if filepath_list is not None:\n",
    "                if file_path not in filepath_list:\n",
    "                    continue\n",
    "            new_anndata = sc.read(file_path)\n",
    "            organ = str(list(new_anndata.obs[organ_label])[0])\n",
    "            numofcells = len(new_anndata.obs_names)\n",
    "            new_anndata = preprocessing(input_adata=new_anndata,if_filter_genes=True,\n",
    "                                if_norm=True,if_hvg=False,n_top_hvgs=None)\n",
    "            print_anndata(input_adata=new_anndata)\n",
    "            num_classes = len(new_anndata.obs[celltype_label].value_counts())\n",
    "            numofcelltypes = num_classes\n",
    "            \n",
    "            feature_len = len(new_anndata.var_names)\n",
    "            print(f'feature_len = {feature_len}')\n",
    "            \n",
    "            print_memory_used()\n",
    "            \n",
    "            new_anndata.obs['cellname'] = new_anndata.obs_names\n",
    "            new_anndata.obs['celltype'] = new_anndata.obs[celltype_label]\n",
    "            new_anndata_train, new_anndata_valid, new_anndata_test = split_anndata_by_celltype_to_train_valid_test(\n",
    "                new_anndata=new_anndata, celltype_label=celltype_label, split_ratio=split_ratio, random_seed=random_seed,\n",
    "            )\n",
    "            del new_anndata\n",
    "            gc.collect()\n",
    "            if if_df is True:\n",
    "                train_data,train_label,valid_data,valid_label,test_data,test_label,mapping_1,mapping_2,category_counts = generate_datas_and_labels(\n",
    "                    input_adatas = [new_anndata_train, new_anndata_valid, new_anndata_test],\n",
    "                    the_length = int(math.sqrt(feature_len) + 1),\n",
    "                    if_balance_train_data = if_balance_train_data,\n",
    "                    random_sample_num = random_sample_num,\n",
    "                )\n",
    "                del new_anndata_train, new_anndata_valid, new_anndata_test\n",
    "                gc.collect()\n",
    "            if if_feature_len is True:\n",
    "                train_data = train.iloc[:,:feature_len]\n",
    "                valid_data = valid.iloc[:,:feature_len]\n",
    "                test_data = test.iloc[:,:feature_len]\n",
    "                \n",
    "            model, accuracy, macro_precision, macro_recall, macro_f1_score = train_ResNet2(\n",
    "                train_data=train_data,train_label=train_label,\n",
    "                valid_data=valid_data,valid_label=valid_label,\n",
    "                test_data=test_data,test_label=test_label,\n",
    "                the_batch_size=the_batch_size,num_classes=num_classes,\n",
    "                lr=lr,dropout_ratio1=dropout_ratio1,dropout_ratio2=dropout_ratio2,dropout_ratio3=dropout_ratio3,\n",
    "                epochs=epochs,gpu_code=gpu_code,the_num_workers=the_num_workers,\n",
    "                optimizer=the_optimizer,momentum=the_momentum,\n",
    "            )\n",
    "            \n",
    "            renames.append(file.replace('.h5ad',''))\n",
    "            reorgans.append(organ)\n",
    "            renumofcells.append(numofcells)\n",
    "            renumofcelltypes.append(numofcelltypes)\n",
    "            rerandomseeds.append(random_seed)\n",
    "            reaccuracys.append(accuracy)\n",
    "            remacrof1scores.append(macro_f1_score)\n",
    "            # break\n",
    "        except Exception:\n",
    "            renames.append(file.replace('.h5ad',''))\n",
    "            reorgans.append('-1')\n",
    "            renumofcells.append(-1)\n",
    "            renumofcelltypes.append(-1)\n",
    "            rerandomseeds.append(-1)\n",
    "            reaccuracys.append(-1)\n",
    "            remacrof1scores.append(-1)\n",
    "            # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278a1d0-a66f-426f-befb-a2a0fde3fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(renames)):\n",
    "    print(str(renames[i])+'\\t'+str(reorgans[i])+'\\t'+ \\\n",
    "          str(renumofcells[i])+'\\t'+str(renumofcelltypes[i])+'\\t'+ \\\n",
    "          str(rerandomseeds[i])+'\\t'+str(reaccuracys[i])+'\\t'+str(remacrof1scores[i]))\n",
    "\n",
    "with open('resnet2-24-1-15-try-all-renames.pkl', 'wb') as file:\n",
    "    pickle.dump(renames, file)\n",
    "with open('resnet2-24-1-15-try-all-reorgans.pkl', 'wb') as file:\n",
    "    pickle.dump(reorgans, file)\n",
    "with open('resnet2-24-1-15-try-all-renumofcells.pkl', 'wb') as file:\n",
    "    pickle.dump(renumofcells, file)\n",
    "with open('resnet2-24-1-15-try-all-renumofcelltypes.pkl', 'wb') as file:\n",
    "    pickle.dump(renumofcelltypes, file)\n",
    "with open('resnet2-24-1-15-try-all-rerandomseeds.pkl', 'wb') as file:\n",
    "    pickle.dump(rerandomseeds, file)\n",
    "with open('resnet2-24-1-15-try-all-reaccuracys.pkl', 'wb') as file:\n",
    "    pickle.dump(reaccuracys, file)\n",
    "with open('resnet2-24-1-15-try-all-remacrof1scores.pkl', 'wb') as file:\n",
    "    pickle.dump(remacrof1scores, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (PCmaster_anno_win)",
   "language": "python",
   "name": "pcmaster_anno_win"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
